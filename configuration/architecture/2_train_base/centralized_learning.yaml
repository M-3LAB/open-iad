# centralzied learning
learning_mode: 'centralized'
# different training
num_task: 15 
# continual learning
continual: false
# fewshot learning
fewshot: false 
fewshot_exm: 5 
# training
batch_size: 32 
#batch_limit: 2
gpu_id: 0
gpu_ids:
- '01234567'
lr: 0.0001
num_epoch: 1 
decay_epoch: 8 
seed: 123 
num_workers: 8
# work file
work_dir: ./work_dir
save_log: true
debug: false 
# save and load model
save_model: false
load_model: false
load_model_dir: None
# optimizer
beta1: 0.5
beta2: 0.999
# chosen task ids
chosen_train_task_ids:
  - 3
chosen_test_task_id: 0